{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tensorflow Tutorial\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph Construction and Session\n",
    "- Tensorflow는 기본적으로 값과 연산을 가지는 node들로 구성된 Directed Acyclic Graph를 형성하여 구현되는 구조를 가진다. \n",
    "\n",
    "### 1.1. tf.constant() & tf.Session()\n",
    "- tf.constant()는 변하지 않은 값을 가지는 node를 생성한다.\n",
    "- tf.constant(value, dtype=None, shape=None, name=\"Const', verify_shape=False)\n",
    "- tf.sesstion()은 형성된 graph를 GPU/CPU에 올려 실행(계산)하는 역할을 한다.\n",
    "\n",
    "#### 1.1.1. compute '3+4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3과 4를 담는 node를 각각 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'Const:0' shape=() dtype=float32>, <tf.Tensor 'Const_1:0' shape=() dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.Session 을 통해서 graph의 node를 run하면 node 값을 계산하고 결과를 return된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print \"node3: \",node3\n",
    "print \"sess.run(node3): \",sess.run(node3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.1.2. Compute ‘((3+4)^2)*5’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 + 4 =  7.0\n",
      "(3 + 4)^2 =  49.0\n",
      "((3 + 4)^2) * 5 =  245.0\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0, tf.float32)\n",
    "node3 = tf.add(node1, node2)\n",
    "node4 = tf.constant(2.0, tf.float32)\n",
    "node5 = tf.pow(node3, node4)\n",
    "node6 = tf.constant(5.0, tf.float32)\n",
    "node7 = tf.multiply(node5, node6)\n",
    "\n",
    "print \"3 + 4 = \",sess.run(node3)\n",
    "print \"(3 + 4)^2 = \",sess.run(node5)\n",
    "print \"((3 + 4)^2) * 5 = \",sess.run(node7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2. tf.placeholder()\n",
    "- tf.placeholder는 graph를 run할 때 값을 입력해주어야 하는 node이다.\n",
    "- 함수의 input 즉 f(x)에서 x의 역할과도 같다.\n",
    "\n",
    "#### 1.2.1. Compute 'x+y'\n",
    "- tf.placeholder를 이용하여 sess.run할때마다 x,y에 input값을 주어 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "adder = x + y\n",
    "\n",
    "z1 = sess.run(adder, {x: 3, y: 4.5})\n",
    "print(z1)\n",
    "\n",
    "z2 = sess.run(adder, {x: [1,3], y: [2, 4]})\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2.2. Compute '||Ax-y||'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*x = \n",
      "[[  5.]\n",
      " [ 11.]]\n",
      "|Ax-y| = \n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "A = tf.constant([[1., 2.], [3., 4.]])\n",
    "y_ = tf.matmul(A,x)\n",
    "diff = y - y_\n",
    "err = tf.norm(diff)\n",
    "\n",
    "# should pass x to 2D matrix shape\n",
    "print \"A*x = \\n\",sess.run(y_, {x: [[1.], [2.]]})\n",
    "print \"|Ax-y| = \\n\",sess.run(err, {x: [[1.], [2.]], y:[[8.], [7.]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Variables\n",
    "- tf.Variable()는 tf.constant()와 같이 값이 고정된 것이 아니고 값을 변경할 수 있는 node이다.\n",
    "- 예를들어, f(x) = 3*x^2 + 2*x + 1 라 했을 때 x는 placeholder이고 3,2,1은 값을 변경할 상수라 하면 tf.constant이다.\n",
    "- 그런데, f(x) = a*x^2 + b*x + c 에서 필요에 따라 a,b,c 값을 변경해줄 수 있다면, 이들은 tf.Variable로 정의한다.\n",
    "- tf.Variable은 결국 function(graph)의 parameter이고, learning이 가능하다.\n",
    "- tf.Variable(initial_value, name=optional_name)\n",
    "\n",
    "### 2.1. Learning a Linear Model\n",
    "- 이 chapter에서는 주어진 데이터로부터 함수 f(x)를 선형회귀하는 모델 Wx+b 를 learning한다. \n",
    "- Initializer는 텐서플로우 variable들의 정의된 초기값을 부여해준다. \n",
    "- tf.global_initializer는 graph 내의 정의된 모든 variable을 초기화시키는 연산자.\n",
    "- 초기화 연산자를 sess.run()으로 실행시켜 초기화해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sess.run에 계산할 node 이름과 거기에 필요한 placeholder 값을 주어 결과를 뱉는다.\n",
    "- 하지만, variable들을 initialize하지 않고 graph를 run하면 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable)]]\n\t [[Node: add_1/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_6_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Variable/read', defined at:\n  File \"/home/daniel/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/daniel/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-5ea878529ef4>\", line 1, in <module>\n    W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 316, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1338, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable)]]\n\t [[Node: add_1/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_6_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cef581f4ca91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# without initializing variable, running graph causes error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable)]]\n\t [[Node: add_1/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_6_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Variable/read', defined at:\n  File \"/home/daniel/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/daniel/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-5ea878529ef4>\", line 1, in <module>\n    W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 316, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1338, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable)]]\n\t [[Node: add_1/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_6_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# without initializing variable, running graph causes error\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- global_variables_initializer로 graph에 정의된 모든 variable들을 초기화하고 다시 시도하면 성공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run variable initializer op, before running the graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "# Then, it works\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss 함수는 squared error function 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  23.66\n"
     ]
    }
   ],
   "source": [
    "# Define loss\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "print \"loss : \",sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model을 training하기 전에, variable의 값 변경하는 법을 소개..\n",
    "- tf.assign(variable, modified_value) 하여 variable의 값을 modified_value로 바꾸는 연산자 생성하고\n",
    "- sess.run()에 넘겨 값을 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assigning, W :  0.3 b :  -0.3\n",
      "After assigning, W :  -1.0 b :  1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# assign different value to the variables\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])   # These are optimal parameters\n",
    "\n",
    "print \"Before assigning, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]\n",
    "sess.run([fixW, fixb])\n",
    "print \"After assigning, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]\n",
    "\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializer을 다시 run하면 처음에 정의된 값으로 초기화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ater initializing again, W :  0.3 b :  -0.3\n"
     ]
    }
   ],
   "source": [
    "# Go back to the initial value\n",
    "sess.run(init)\n",
    "print \"Ater initializing again, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.train에 정의된 optimizer중 하나를 이용하여 variable들을 learning한다.\n",
    "- 예시에서는 일반적인 Stochastic Gradient Descent (SGD) optimizer 이용.\n",
    "- Optimizer는 loss를 minimizing하는 방향으로 배우도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  100  loss :  0.149934\n",
      "iteration :  200  loss :  0.0134721\n",
      "iteration :  300  loss :  0.00121051\n",
      "iteration :  400  loss :  0.000108768\n",
      "iteration :  500  loss :  9.77349e-06\n",
      "iteration :  600  loss :  8.77956e-07\n",
      "iteration :  700  loss :  7.88489e-08\n",
      "iteration :  800  loss :  7.08758e-09\n",
      "iteration :  900  loss :  6.4195e-10\n",
      "iteration :  1000  loss :  5.77707e-11\n",
      "Learned model parameters, \t W :  -0.999997 , b :  0.999991\n",
      "Optimal parameters are,   \t W :  -1 \t, b :  1\n"
     ]
    }
   ],
   "source": [
    "# Learn the model using optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "for i in range(1000):\n",
    "    _, l = sess.run([train, loss], {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "    if (i+1) % 100 == 0:\n",
    "        print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    \n",
    "print \"Learned model parameters, \\t W : \",sess.run(W)[0],\", b : \",sess.run(b)[0]\n",
    "print \"Optimal parameters are,   \\t W :  -1 \\t, b :  1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2. Initializing Variables\n",
    "- 여러가지 initialize 방법들 소개\n",
    "\n",
    "#### 2.2.1. tf.global_variable_initializer()\n",
    "- model의 모든 variable 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.2 ,\t W2 : 0.4 ,\t b : -0.3\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable([.2], tf.float32)\n",
    "W2 = tf.Variable([.4], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 tf.variables_initialize()\n",
    "- tf.variables_initialize(var_list)는 var_list (variables들의 list)에 포함된 variables들만 initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.1 ,\t W2 : 0.1 ,\t b : -0.1\n"
     ]
    }
   ],
   "source": [
    "# 위에 정의된 variable들의 값을 바꾼다.\n",
    "assn_W1 = W1.assign([.1])\n",
    "assn_W2 = W2.assign([.1])\n",
    "assn_b = b.assign([-.1])\n",
    "\n",
    "sess.run([assn_W1, assn_W2, assn_b])\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.2 ,\t W2 : 0.1 ,\t b : -0.3\n"
     ]
    }
   ],
   "source": [
    "# W1, b만 초기값으로 initialize한다.\n",
    "init_W = tf.variables_initializer([W1, b])\n",
    "# init_W = tf.variables_initializer(set(tf.global_variables())-W2)\n",
    "sess.run(init_W)\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Variable.initialized_value()\n",
    "- 두 변수 A, B를 동일한 값으로 초기화시킬 때, Variable의 initialized_value()를 이용한다.\n",
    "- initialized_value는 그 변수의 초기화 값을 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :\n",
      "[[ 0.09768371 -0.20012711  0.09830294]\n",
      " [-0.07883383  0.08372327 -0.48926619]] \n",
      "B :\n",
      "[[ 0.09768371 -0.20012711  0.09830294]\n",
      " [-0.07883383  0.08372327 -0.48926619]] \n",
      "C :\n",
      "[[ 0.19536741 -0.40025422  0.19660588]\n",
      " [-0.15766767  0.16744654 -0.97853237]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.Variable(tf.random_normal([2,3], stddev=0.35), name=\"A\")\n",
    "# A와 같은 초기 값을 가지는 변수 B를 만든다.\n",
    "B = tf.Variable(A.initialized_value(), name=\"B\")\n",
    "# A의 초기값의 정확히 2배의 초기값을 가지는 변수 C를 만든다.\n",
    "C = tf.Variable(A.initialized_value() * 2.0, name=\"C\")\n",
    "\n",
    "init_ABC = tf.variables_initializer(set([A,B,C]))\n",
    "sess.run(init_ABC)\n",
    "print \"A :\\n\",sess.run(A),\"\\nB :\\n\",sess.run(B),\"\\nC :\\n\",sess.run(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Saving and Loading Variables\n",
    "- 배운 모델을 저장하거나, 배워진 모델을 읽어들일 때 필요한 과정.\n",
    "- tf.train.Saver()을 사용한다.\n",
    "- 모든 변수가 아니라, 지정한 몇 개의 변수만 저장하고 불러올 수도 있다.\n",
    "- key : 저장/불러올 변수 이름의 이름, value : 저장/불러올 변수 를 가지는 python dictionary를 만들어서 train.Saver의 input으로 넘긴다.\n",
    "- train.Saver의 input이 없을 때는 default로 graph 내의 모든 변수들을 save/restore한다. \n",
    "- graph 내의 모든 변수들을 보려면 global_variables() 함수를 사용하여, print(global_bvariables()) 등을 이용.\n",
    "\n",
    "#### 2.4.1 Save model : all variables\n",
    "- tf.train.Saver.save()를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  20  loss :  14.5374\n",
      "iteration :  40  loss :  13.6048\n",
      "iteration :  60  loss :  12.6724\n",
      "iteration :  80  loss :  11.7404\n",
      "iteration :  100  loss :  10.8088\n",
      "iteration :  120  loss :  9.87758\n",
      "iteration :  140  loss :  8.94705\n",
      "iteration :  160  loss :  8.01737\n",
      "iteration :  180  loss :  7.08886\n",
      "iteration :  200  loss :  6.16204\n",
      "v1 :\n",
      "[[-0.40074223 -0.08862444]\n",
      " [ 0.30423781  0.03049548]] \n",
      "v2 :\n",
      "[[ 0.45579135]\n",
      " [-0.83171564]]\n",
      "Model (variables v1, v2) saved in file: model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Variable 생성\n",
    "v1 = tf.Variable(tf.random_normal([2,2]), name=\"v1\")\n",
    "v2 = tf.Variable(tf.random_normal([2,1]), name=\"v2\")\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# tf.train.Saver() 을 먼저 생성. 저장할 변수들 pass \n",
    "saver = tf.train.Saver({\"w1\": v1, \"w2\": v2})\n",
    "# saver = tf.train.Saver([v1, v2])   # name은 default(v.op.name)으로 들어감\n",
    "# saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
    "\n",
    "# 모델 생성, 변수 초기화, training, 모델 저장.\n",
    "with tf.Session() as sess:\n",
    "    # model 생성\n",
    "    loss = tf.norm(tf.matmul(v1,x)+v2-y)\n",
    "    \n",
    "    # 변수 초기화\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    for i in range(200):\n",
    "        _, l = sess.run([train, loss], {x:[[1.,2.,3.,4.],[0.,1.,2.,3.]], y:[[0.,-1.,-2.,-3.],[-1.,-2.,-3.,-4.]]})\n",
    "        if (i+1) % 20 == 0:\n",
    "            print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    \n",
    "    # 디스크에 변수 저장\n",
    "    save_path = saver.save(sess, \"model.ckpt\")  # 세션과 저장 파일 이름 지정\n",
    "    print \"v1 :\\n\",sess.run(v1), \"\\nv2 :\\n\", sess.run(v2)\n",
    "    print(\"Model (variables v1, v2) saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Load model : all variables\n",
    "- 변수들을 저장된 값을 불러 초기화한다. \n",
    "- 이 경우 restore된 변수들은 따로 initializer로 초기화할 필요 없다.\n",
    "- tf.train.Saver.restore()를 사용\n",
    "- 밑의 예시는 위 예시에서 저장된 checkpoint부터 시작하여 training을 이어나가는 과정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "Model restored.\n",
      "w1 :\n",
      "[[-0.40074223 -0.08862444]\n",
      " [ 0.30423781  0.03049548]] \n",
      "w2 :\n",
      "[[ 0.45579135]\n",
      " [-0.83171564]]\n",
      "iteration :  20  loss :  5.23778\n",
      "iteration :  40  loss :  4.31766\n",
      "iteration :  60  loss :  3.4049\n",
      "iteration :  80  loss :  2.5071\n",
      "iteration :  100  loss :  1.64674\n",
      "iteration :  120  loss :  0.913126\n",
      "iteration :  140  loss :  0.574\n",
      "iteration :  160  loss :  0.521094\n",
      "iteration :  180  loss :  0.494606\n",
      "iteration :  200  loss :  0.468901\n"
     ]
    }
   ],
   "source": [
    "# Variable 생성\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name=\"w1\")\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name=\"w2\")\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# restorer를 정의하여 restore할 \n",
    "restorer = tf.train.Saver({\"w1\": w1, \"w2\": w2})\n",
    "with tf.Session() as sess:\n",
    "    # model 생성\n",
    "    loss = tf.norm(tf.matmul(w1,x)+w2-y)\n",
    "    \n",
    "    # checkpoint 파일로부터 변수값 읽어와서 변수 초기화\n",
    "    restorer.restore(sess, \"model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    print \"w1 :\\n\",sess.run(w1), \"\\nw2 :\\n\", sess.run(w2)\n",
    "    \n",
    "    # training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    for i in range(200):\n",
    "        _, l = sess.run([train, loss], {x:[[1.,2.,3.,4.],[0.,1.,2.,3.]], y:[[0.,-1.,-2.,-3.],[-1.,-2.,-3.,-4.]]})\n",
    "        if (i+1) % 20 == 0:\n",
    "            print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- restore되지 않은 변수는 꼭 초기화를 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "Initialize w1, w2 by restoring saved values\n",
      "w1 :  [[-0.40074223 -0.08862444]\n",
      " [ 0.30423781  0.03049548]] \n",
      "w2 :  [[ 0.45579135]\n",
      " [-0.83171564]]\n",
      "\n",
      "w3 :  [[-0.43124294]\n",
      " [ 0.24296828]] \n",
      "w4 :  [[ 0.15348169]\n",
      " [ 0.59824109]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2,2]), name=\"w1\")\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name=\"w2\")\n",
    "w3 = tf.Variable(tf.random_normal([2,1]), name=\"w3\")\n",
    "w4 = tf.Variable(tf.random_normal([2,1]), name=\"w4\")\n",
    "\n",
    "restorer = tf.train.Saver({\"w1\": w1, \"w2\": w2})\n",
    "with tf.Session() as sess:\n",
    "    restorer.restore(sess, \"model.ckpt\")\n",
    "    print(\"Initialize w1, w2 by restoring saved values\")\n",
    "\n",
    "    # restore 되지 않은 w3는 초기화\n",
    "    init_34 = tf.variables_initializer([w3, w4])\n",
    "    sess.run(init_34)\n",
    "    \n",
    "    print \"w1 : \",sess.run(w1), \"\\nw2 : \", sess.run(w2)\n",
    "    print \"\\nw3 : \",sess.run(w3), \"\\nw4 : \", sess.run(w4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sharing Variables\n",
    "- Variable은 선언할 때 마다 새로 생성되어 덮어씌어진다는 특징이 있는데, 이미 만든 variable은 새로 만들지 않고 기존의 것을 재사용하게 하도록 하는 방법이 필요할 때가 있다.\n",
    "- 밑의 예시는 convolution layer를 포함한 model인데, 두가지의 문제점이 있다.\n",
    "    - 첫째로, 함수 내의 모델이 고정되어있어 모델의 layer를 추가하려면 새로운함수를 만들거나, main함수에 직접 넣어주어야 하는 비효율이 발생한다.\n",
    "    - 둘째로, 모델에 input을 넣어줄 때마다 새로운 variable이 생성되어, input에 따라 다른 filter를 통과하게."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 3, 32]), name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv1 = tf.nn.conv2d(input_images, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    conv2 = tf.nn.conv2d(relu1, conv2_weights,  strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + conv2_biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "\n",
    "image1 = tf.expand_dims(tf.constant(misc.imread('acoustic-guitar-player.jpg'), tf.float32), 0)\n",
    "image2 = tf.expand_dims(tf.constant(misc.imread('iris.jpg'), tf.float32), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 언급한 두 번째 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of created variables : \n",
      "4\n",
      "Number of created variables : \n",
      "8\n"
     ]
    }
   ],
   "source": [
    "temp = tf.global_variables()\n",
    "# First call creates one set of 4 variables.\n",
    "result1 = my_image_filter(image1)\n",
    "print \"Number of created variables : \\n\",len(set(tf.global_variables())-set(temp))\n",
    "\n",
    "# Another set of 4 variables is created in the second call.\n",
    "result2 = my_image_filter(image2)\n",
    "print \"Number of created variables : \\n\",len(set(tf.global_variables())-set(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.placeholder를 이용하여 sess.run 할때마다 input을 넘겨주거나, variable들을 dictionary로 따로 함수 밖에 만들어놓은 뒤 함수에서 refer하면 두 번째 문제는 해결되지만, 첫번째 문제는 여전히 해결하지 못한다.\n",
    "- tf.variable_scope 와 tf.get_variable을 이용하여 두 문제를 효율적으로 해결해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. tf.get_variable() & tf.variable_scope()\n",
    "- get_variable은 이미 존재하는 이름의 변수는 더 이상 만들지 않는다. \n",
    "- 같은 이름의 변수를 만들려고 하면 error가 발생\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable v already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-53-cb15e7b01923>\", line 3, in <module>\n    v = tf.get_variable(\"v\", [1])\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-cb15e7b01923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable v already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-53-cb15e7b01923>\", line 3, in <module>\n    v = tf.get_variable(\"v\", [1])\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/daniel/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "v = tf.get_variable(\"v\", [1])\n",
    "v1 = tf.get_variable(\"v\", [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable_scope는 scope 내의 정의된 변수 이름에 scope 이름을 prefix로 붙여 구분지어준다.\n",
    "- scope안에 다른 scope을 recursive하게 정의가 가능. 변수명의 prefix는 가장 밖부터 쌓이게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/v:0\n",
      "foo/bar/v:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "print(v.name)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "print(v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable scope를 정의하고, 그 안에서 정의된 variable은 다시 정의되었을때 재사용하도록 지정할 수 있다.\n",
    "- variable_scope 내의 option reuse를 True로 두면 scope 내의 변수들은 재사용된다.\n",
    "- tf.get_variable_scope().reuse_variables() 호출 이후 scope 내의 변수들은 재사용된다.\n",
    "- 한 scope내에서 변수들을 재사용하도록 변경 설정하였으면, 다시 재사용 못하도록 물르는 것은 불가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 is v     # v1 = v.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):                  # tf.get_variable_scope().reuse == False\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "    tf.get_variable_scope().reuse_variables()   # tf.get_variable_scope().reuse == True\t\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 is v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scope 내에서 reuse할 변수와 그렇지 않을 변수들을 나눠 scope로 구분할 수 있다.\n",
    "- 변수의 재사용성 여부는 내부 scope에 그대로 상속된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuse root ?    False\n",
      "reuse root/foo ?    False\n",
      "reuse root/foo ?    True\n",
      "reuse root/foo/bar ?    True\n",
      "reuse root ?    False\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"root\"):\n",
    "    # At start, the scope is not reusing.\n",
    "    print \"reuse root ?   \", tf.get_variable_scope().reuse\n",
    "    with tf.variable_scope(\"foo\"):\n",
    "        # Opened a sub-scope, still not reusing.\n",
    "        print \"reuse root/foo ?   \", tf.get_variable_scope().reuse\n",
    "    with tf.variable_scope(\"foo\", reuse=True):\n",
    "        # Explicitly opened a reusing scope.\n",
    "        print \"reuse root/foo ?   \", tf.get_variable_scope().reuse\n",
    "        with tf.variable_scope(\"bar\"):\n",
    "            # Now sub-scope inherits the reuse flag.\n",
    "            print \"reuse root/foo/bar ?   \", tf.get_variable_scope().reuse\n",
    "    # Exited the reusing scope, back to a non-reusing one.\n",
    "    print \"reuse root ?   \", tf.get_variable_scope().reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\") as foo_scope:\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(foo_scope):\n",
    "    w = tf.get_variable(\"w\", [1])\n",
    "with tf.variable_scope(foo_scope, reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "    w1 = tf.get_variable(\"w\", [1])\n",
    "assert v1 is v\n",
    "assert w1 is w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내부 scope에서 외부 scope으로의 접근이 가능하다. \n",
    "- 이때 기존 scope의 위치는 무시하고, 접근한 scope의 위치와 상태를 그대로 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo_scope.name : foo\n",
      "other_scope.name : bar/baz\n",
      "foo_scope2.name : foo\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\") as foo_scope:\n",
    "    print(\"foo_scope.name : %s\" % foo_scope.name)\n",
    "with tf.variable_scope(\"bar\"):\n",
    "    with tf.variable_scope(\"baz\") as other_scope:\n",
    "        print(\"other_scope.name : %s\" % other_scope.name)\n",
    "        with tf.variable_scope(foo_scope) as foo_scope2:\n",
    "            print(\"foo_scope2.name : %s\" % foo_scope2.name)  # Not changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable scope의 또 다른 기능은, get_variable의 default initializer를 'initializer' 옵션으로 미리 지정해줄 수 있다.\n",
    "- 따로 정의를 해주지 않는 이상, default initializer는 내부 scope들에게도 그대로 상속된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()\n",
    "sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo_v : 0.4\n",
      "foo_w : 0.3\n",
      "foo_bar_v : 0.4\n",
      "foo_baz_v : -1.1904\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"foo\", initializer=tf.constant_initializer(0.4)):\n",
    "    foo_v = tf.get_variable(\"v\", [1])\n",
    "    foo_w = tf.get_variable(\"w\", [1], initializer=tf.constant_initializer(0.3))\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        foo_bar_v = tf.get_variable(\"v\", [1])    \n",
    "    with tf.variable_scope(\"baz\", initializer=tf.random_normal_initializer()):\n",
    "        foo_baz_v = tf.get_variable(\"v\", [1])\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"foo_v : %.1f\" % foo_v.eval(session=sess)) # Default initializer as set above.\n",
    "    print(\"foo_w : %.1f\" % foo_w.eval(session=sess)) # Specific initializer overrides the default.\n",
    "    print(\"foo_bar_v : %.1f\" % foo_bar_v.eval(session=sess)) # Inherited default initializer.\n",
    "    print(\"foo_baz_v : %.4f\" % foo_baz_v.eval(session=sess))  # Changed default initializer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solving the previous problem\n",
    "- get variable로 variable을 정의하고\n",
    "- layer마다 variable scope을 달리하여 weight들을 구분해준다. \n",
    "- layer 생성할 때마다 layer 추가하는 함수를 호출하여, 코드의 재사용성을 높일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\", “biases”.\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, \tinitializer=tf.random_normal_initializer())\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 3, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter(image1)\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Name Scope\n",
    "- name scope은 tf.Variable 로 정의한 variable과 operation의 이름을 관리해준다.\n",
    "- get_variable에 의해 정의한 variable은 무시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/add\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    x = 1.0 + tf.get_variable(\"v\", [1])\n",
    "print(x.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v name : foo/v:0\n",
      "w name : foo/bar/Variable:0\n",
      "x operation name : foo/bar/add\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.name_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        w = tf.Variable([.1], tf.float32)\n",
    "        x = 1.0 + v\n",
    "print(\"v name : %s\" % v.name)\n",
    "print(\"w name : %s\" % w.name)\n",
    "print(\"x operation name : %s\" % x.op.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
